{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8454731",
   "metadata": {},
   "source": [
    "# KG Index and VectorStore Index on Hotpot QA with LlamaIndex\n",
    "\n",
    "\n",
    "This notebook demonstrates the following:\n",
    "\n",
    "    1) Building a custom `KnowledgeGraphIndex` for extracted triples.\n",
    "    2) How to build a customized `BaseRetriever` to retrieve KG triples and text documents jointly.\n",
    "    3) Automated evaluation of KG RAG based QA using LLMs for evaluation on hotpot qa.\n",
    "    \n",
    "This notebook compares a `VectorIndexRetriever`, `KGTableRetriever`, and joint Retriever for RAG on the `hotpot_qa` dataset. The triples can be extracted following `hotpot_qa_extraction.ipynb` or by running `hotpot_qa_kgs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "011ff10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walder2/torch_geometric_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "open_ai_key = '...'\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_key\n",
    "\n",
    "sys.path = ['/Users/walder2/kg_uq/'] + sys.path\n",
    "path_to_data = '/Users/walder2/kg_uq/hotpot_qa_data'\n",
    "\n",
    "from hotpot_qa_data.hotpot_data_load import load_hotpot_kgs\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    "    QueryBundle,\n",
    "    Response\n",
    ")\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index.schema import NodeWithScore\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    KGTableRetriever,\n",
    ")\n",
    "\n",
    "#Evaluators\n",
    "from llama_index.evaluation import CorrectnessEvaluator, BaseEvaluator\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import time\n",
    "import asyncio \n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3735a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg, query_answer = load_hotpot_kgs(path_to_data=path_to_data, query_answer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00f20f",
   "metadata": {},
   "source": [
    "### Look at the data\n",
    "\n",
    "`doc_id` is the id for the group (question group id), `sub_idx` is the id of the subgraph extracted for context entry `j` for a particular question. Some of the text is messy, we can clean that up at a later time. \n",
    "\n",
    "Also note that the `file_path` is included. This helps track where the triples came from. It is important to know if you want to track down top-matching subgraphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6cd8c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>head_type</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "      <th>tail_type</th>\n",
       "      <th>file_path</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sub_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>isFirstPrivateFMStationIn</td>\n",
       "      <td>India</td>\n",
       "      <td>place</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>wasStartedOn</td>\n",
       "      <td>3 July 2001</td>\n",
       "      <td>event</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>broadcastsOn</td>\n",
       "      <td>91.1 megahertz</td>\n",
       "      <td>measurement</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>broadcastsFrom</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>place</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>broadcastsFrom</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>place</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>2014 Liqui Moly Bathurst 12 Hour</td>\n",
       "      <td>event</td>\n",
       "      <td>wasHeldOn</td>\n",
       "      <td>9 February 2014</td>\n",
       "      <td>date</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>2014 Liqui Moly Bathurst 12 Hour</td>\n",
       "      <td>event</td>\n",
       "      <td>wasTheTwelfthRunningOf</td>\n",
       "      <td>Bathurst 12 Hour</td>\n",
       "      <td>event</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>2014 Liqui Moly Bathurst 12 Hour</td>\n",
       "      <td>event</td>\n",
       "      <td>included</td>\n",
       "      <td>GT3 cars</td>\n",
       "      <td>thing</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>2014 Liqui Moly Bathurst 12 Hour</td>\n",
       "      <td>event</td>\n",
       "      <td>included</td>\n",
       "      <td>GT4 cars</td>\n",
       "      <td>thing</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>2014 Liqui Moly Bathurst 12 Hour</td>\n",
       "      <td>event</td>\n",
       "      <td>included</td>\n",
       "      <td>Group 3E Series Production Cars</td>\n",
       "      <td>thing</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1031 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  head head_type                   relation  \\\n",
       "0                           Radio City     place  isFirstPrivateFMStationIn   \n",
       "1                           Radio City     place               wasStartedOn   \n",
       "2                           Radio City     place               broadcastsOn   \n",
       "3                           Radio City     place             broadcastsFrom   \n",
       "4                           Radio City     place             broadcastsFrom   \n",
       "...                                ...       ...                        ...   \n",
       "1026  2014 Liqui Moly Bathurst 12 Hour     event                  wasHeldOn   \n",
       "1027  2014 Liqui Moly Bathurst 12 Hour     event     wasTheTwelfthRunningOf   \n",
       "1028  2014 Liqui Moly Bathurst 12 Hour     event                   included   \n",
       "1029  2014 Liqui Moly Bathurst 12 Hour     event                   included   \n",
       "1030  2014 Liqui Moly Bathurst 12 Hour     event                   included   \n",
       "\n",
       "                                 tail    tail_type  \\\n",
       "0                               India        place   \n",
       "1                         3 July 2001        event   \n",
       "2                      91.1 megahertz  measurement   \n",
       "3                              Mumbai        place   \n",
       "4                           Bengaluru        place   \n",
       "...                               ...          ...   \n",
       "1026                  9 February 2014         date   \n",
       "1027                 Bathurst 12 Hour        event   \n",
       "1028                         GT3 cars        thing   \n",
       "1029                         GT4 cars        thing   \n",
       "1030  Group 3E Series Production Cars        thing   \n",
       "\n",
       "                                              file_path  doc_id  sub_idx  \n",
       "0     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "1     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "2     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "3     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "4     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "...                                                 ...     ...      ...  \n",
       "1026  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       9        9  \n",
       "1027  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       9        9  \n",
       "1028  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       9        9  \n",
       "1029  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       9        9  \n",
       "1030  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       9        9  \n",
       "\n",
       "[1031 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780848f3",
   "metadata": {},
   "source": [
    "# Building a KnowledgeGraphIndex from extracted triples\n",
    "\n",
    "Pick an LLM for use, default is gpt-3.5-turbo. `service_context` will help with chunking documents and determining with LLM to call. The path for `documents` should point to `'./hotpot_qa_data/txt_files'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb45b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
    "documents = SimpleDirectoryReader(path_to_data + '/txt_files').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777d708",
   "metadata": {},
   "source": [
    "Define the and empty `KnowledgeGraphIndex`. We will fill this store up with our extracted triples and a reference to the `Node` with contains the document the triples were extracted from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57494acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex(\n",
    "    [],\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "node_parser = SentenceSplitter()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "file_to_node = {node.metadata['file_path']: k for k, node in enumerate(nodes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e727b6dd",
   "metadata": {},
   "source": [
    "We fill up the `KnowledgeGraphIndex` object by passing in triples corresponding to the `Node` they were extracted from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f7a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id in kg['doc_id'].unique():\n",
    "    idx = kg['doc_id'] == doc_id\n",
    "\n",
    "    for sub_id in kg[idx]['sub_idx'].unique():\n",
    "        tmp = kg[np.bitwise_and(idx, kg['sub_idx'] == sub_id)]\n",
    "        for h, r, t, f in zip(tmp['head'], tmp['relation'], tmp['tail'], tmp['file_path']):\n",
    "            kg_index.upsert_triplet_and_node((h, r, t), nodes[file_to_node[f]])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735488d",
   "metadata": {},
   "source": [
    "# Define JointReriever for KG triple and text indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434039ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a4b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675accee",
   "metadata": {},
   "source": [
    "Create a `VectorStoreIndex` object for RAG with just the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7575902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfa844",
   "metadata": {},
   "source": [
    "Now we instantiate a retriever for the KQ, text, which is passed in the `CustomRetriever` object for joint retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff812cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "joint_retriever = JointRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01e498",
   "metadata": {},
   "source": [
    "Create query engines for all three cases: text, KG, and joint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66e39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_query_engine = RetrieverQueryEngine(\n",
    "    retriever=joint_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "# only use triples from the KG\n",
    "kg_keyword_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d68257",
   "metadata": {},
   "source": [
    "# Evaluation of responses with RAG\n",
    "\n",
    "Define a `CorrectnessEvaluator` that checks correctness of response to the query (with answer supplied). There are other tools for out in the wild evaluation of responses. E.g. \n",
    "\n",
    "`ResponseSourceEvaluator` - uses an LLM to decide if the response is similar enough to the sources -- a good measure for hallunication detection.\n",
    "\n",
    "`QueryResponseEvaluator` - uses an LLM to decide if a response is similar enough to the original query -- a good measure for checking if the query was answered.\n",
    "\n",
    "\n",
    "I've defined a function which uses `CorrectnessEvaluator` to check if the response contains an answer suitable for the query, given the correct answer. We can actually write custom evaluators that fit specified guidelines. This will come in handy later when we want to extend the QA to self defined embeddings ect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f780bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "class EvaluateResponse:\n",
    "    def __init__(self, evaluator: BaseEvaluator, query_engine: RetrieverQueryEngine) -> None:\n",
    "        self.eval = evaluator\n",
    "        self.q = query_engine\n",
    "    \n",
    "    async def run_query(self, x: Dict[str, str]):\n",
    "        try:\n",
    "            return await self.q.aquery(x['query'])\n",
    "        except:\n",
    "            return Response(response=\"Error, query failed.\")\n",
    "        \n",
    "    def evaluate(self, x: List[Dict[str, str]]):\n",
    "            total_correct = 0\n",
    "            all_results = []\n",
    "            for batch_size in range(0, len(x), 5):\n",
    "                batch_x = x[batch_size:batch_size+5]\n",
    "\n",
    "                tasks = [self.run_query(y) for y in batch_x]\n",
    "                responses = asyncio.run(asyncio.gather(*tasks))\n",
    "\n",
    "                for y, res in zip(batch_x, responses):\n",
    "                    eval_result = self.eval.evaluate(query=y['query'], reference=y['answer'], response=res.response)\n",
    "                    total_correct += 1 if eval_result.passing else 0 \n",
    "                    all_results.append(eval_result)\n",
    "                \n",
    "                time.sleep(1)\n",
    "            return total_correct, all_results\n",
    "                                                                        \n",
    "                                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f5f82",
   "metadata": {},
   "source": [
    "Run the retrievers on the queries and check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1169a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG evaluation complete...\n",
      "Text evaluation complete...\n",
      "Joint evaluation complete...\n"
     ]
    }
   ],
   "source": [
    "evaluator = CorrectnessEvaluator(service_context=service_context)\n",
    "kg_tot, kg_res = EvaluateResponse(evaluator, kg_keyword_query_engine).evaluate(query_answer)\n",
    "print('KG evaluation complete...')\n",
    "time.sleep(1)\n",
    "txt_tot, txt_res = EvaluateResponse(evaluator, vector_query_engine).evaluate(query_answer)\n",
    "print('Text evaluation complete...')\n",
    "time.sleep(1)\n",
    "joint_tot, joint_res = EvaluateResponse(evaluator, joint_query_engine).evaluate(query_answer)\n",
    "print('Joint evaluation complete...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccf2eb",
   "metadata": {},
   "source": [
    "Take a look at the results. The LLM will tell us if the retrieval is correct and some feedback on why the response was deemed correct or incorrect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a35c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Query: \"Which magazine was started first Arthur's Magazine or First for Women?\"\n",
      "Answer \"Arthur's Magazine\"\n",
      "\n",
      "KG (True): \"Arthur's Magazine was started first.\"\n",
      "Feedback: 'The generated answer is relevant and fully correct. It provides the correct information and is concise. The only improvement could be to include the name of the other magazine mentioned in the query, \"First for Women\", to make the answer more complete.'\n",
      "\n",
      "Text (True): \"Arthur's Magazine was started first.\"\n",
      "Feedback: \"The generated answer is relevant and fully correct. It provides the correct information that Arthur's Magazine was started first. However, it could be improved by providing more context or additional details about the magazine.\"\n",
      "\n",
      "Joint (True): \"Arthur's Magazine was started before First for Women.\"\n",
      "Feedback: \"The generated answer is relevant and correct. It provides the correct information that Arthur's Magazine was started before First for Women. However, it could be improved by providing more context or additional details about the magazines.\"\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'The Oberoi family is part of a hotel company that has a head office in what city?'\n",
      "Answer 'Delhi'\n",
      "\n",
      "KG (True): 'Delhi'\n",
      "Feedback: 'The generated answer is relevant and fully correct, matching the reference answer exactly.'\n",
      "\n",
      "Text (True): 'Delhi.'\n",
      "Feedback: 'The generated answer is both relevant and correct. It matches the reference answer exactly.'\n",
      "\n",
      "Joint (True): 'The Oberoi family is part of a hotel company that has a head office in Delhi.'\n",
      "Feedback: 'The generated answer is both relevant and correct. It provides the exact same information as the reference answer and is concise.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?'\n",
      "Answer 'President Richard Nixon'\n",
      "\n",
      "KG (False): 'Matt Groening named the \"The Simpsons\" character Milhouse after Richard Milhous Nixon.'\n",
      "Feedback: 'The generated answer is relevant to the user query and contains the correct information. However, it is not as concise as the reference answer.'\n",
      "\n",
      "Text (False): 'Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after President Richard Nixon\\'s middle name.'\n",
      "Feedback: \"The generated answer is relevant to the user query and correctly states that Matt Groening named the character Milhouse after President Richard Nixon's middle name. However, it is not as concise as the reference answer.\"\n",
      "\n",
      "Joint (False): 'Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after President Richard Nixon\\'s middle name.'\n",
      "Feedback: \"The generated answer is relevant to the user query and correctly states that Matt Groening named the character Milhouse after President Richard Nixon's middle name. However, it is not as concise as the reference answer.\"\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: \" What nationality was James Henry Miller's wife?\"\n",
      "Answer 'American'\n",
      "\n",
      "KG (False): \"There is no information provided in the given context about James Henry Miller's wife or her nationality.\"\n",
      "Feedback: \"The generated answer is not relevant to the user query as it does not provide any information about James Henry Miller's wife or her nationality.\"\n",
      "\n",
      "Text (False): \"The nationality of James Henry Miller's wife cannot be determined based on the given context information.\"\n",
      "Feedback: 'The generated answer is not relevant to the user query. It states that the nationality cannot be determined, while the user query specifically asks for the nationality.'\n",
      "\n",
      "Joint (False): \"The nationality of James Henry Miller's wife cannot be determined based on the given information.\"\n",
      "Feedback: \"The generated answer is relevant to the user query as it acknowledges that the nationality of James Henry Miller's wife cannot be determined based on the given information. However, it is not fully correct as it does not provide any additional information or context.\"\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Cadmium Chloride is slightly soluble in this chemical, it is also called what?'\n",
      "Answer 'alcohol'\n",
      "\n",
      "KG (False): 'Cadmium Chloride is slightly soluble in water, it is also called aqueous.'\n",
      "Feedback: 'The generated answer is relevant to the user query and correctly states that Cadmium Chloride is slightly soluble in water. However, it incorrectly states that it is also called \"aqueous\" instead of \"alcohol\" as mentioned in the reference answer.'\n",
      "\n",
      "Text (True): 'alcohol'\n",
      "Feedback: 'The generated answer is relevant and fully correct, matching the reference answer exactly.'\n",
      "\n",
      "Joint (True): 'alcohol'\n",
      "Feedback: 'The generated answer is relevant and fully correct, matching the reference answer exactly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?'\n",
      "Answer 'Jonathan Stark'\n",
      "\n",
      "KG (False): 'Henri Leconte won more Grand Slam titles than Jonathan Stark.'\n",
      "Feedback: 'The generated answer is relevant to the user query, but it contains a mistake. The correct answer is Jonathan Stark, not Henri Leconte.'\n",
      "\n",
      "Text (False): 'Henri Leconte won more Grand Slam titles than Jonathan Stark.'\n",
      "Feedback: 'The generated answer is relevant to the user query, but it contains a mistake. The correct answer is Jonathan Stark, not Henri Leconte.'\n",
      "\n",
      "Joint (True): 'Jonathan Stark won more Grand Slam titles than Henri Leconte.'\n",
      "Feedback: 'The generated answer is relevant and fully correct. It provides the correct information that Jonathan Stark won more Grand Slam titles than Henri Leconte. However, it could be improved by providing the exact number of Grand Slam titles won by each player.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: \"Which genus of moth in the world's seventh-largest country contains only one species?\"\n",
      "Answer 'Crambidae'\n",
      "\n",
      "KG (False): \"The genus of moth in the world's seventh-largest country that contains only one species is unknown based on the given context information.\"\n",
      "Feedback: 'The generated answer is not relevant to the user query. It states that the genus of moth is unknown, while the user query specifically asks for the genus that contains only one species.'\n",
      "\n",
      "Text (False): \"The genus of moth in the world's seventh-largest country that contains only one species is Nymphuliella.\"\n",
      "Feedback: 'The generated answer is relevant to the user query as it identifies a genus of moth in the world\\'s seventh-largest country that contains only one species. However, the generated answer is incorrect as it states the genus as \"Nymphuliella\" instead of \"Crambidae\" which is the correct answer provided in the reference answer.'\n",
      "\n",
      "Joint (False): \"Nymphuliella is the genus of moth in the world's seventh-largest country that contains only one species.\"\n",
      "Feedback: \"The generated answer is relevant to the user query as it mentions a genus of moth in the world's seventh-largest country that contains only one species. However, it is not fully correct as the correct genus is Crambidae, not Nymphuliella.\"\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring.'\n",
      "Answer 'Badr Hari'\n",
      "\n",
      "KG (False): 'The person who was once considered the best kick boxer in the world, but has been involved in controversies related to unsportsmanlike conduct in the sport and crimes of violence outside of the ring, is not mentioned in the provided context information.'\n",
      "Feedback: 'The generated answer is not relevant to the user query. It does not provide any information about the person who was once considered the best kick boxer in the world and has been involved in controversies.'\n",
      "\n",
      "Text (True): 'Badr Hari was once considered the best kickboxer in the world, but he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring.'\n",
      "Feedback: 'The generated answer is relevant and fully correct. It provides the correct information about Badr Hari being considered the best kickboxer in the world and his involvement in controversies and crimes. The only minor issue is the use of quotation marks around \"unsportsmanlike conducts\", which is not necessary.'\n",
      "\n",
      "Joint (True): 'Badr Hari was once considered the best kickboxer in the world, but he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring.'\n",
      "Feedback: 'The generated answer is relevant and fully correct. It provides the correct information about Badr Hari being considered the best kickboxer in the world and his involvement in controversies and crimes. The only minor issue is the repetition of the phrase \"in the sport\" in the generated answer, which could have been avoided for better clarity.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in what year?'\n",
      "Answer '2006'\n",
      "\n",
      "KG (False): 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in 1 January 2011.'\n",
      "Feedback: 'The generated answer is relevant to the user query, but it contains a mistake. The correct year is 2006, not 2011.'\n",
      "\n",
      "Text (True): 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in September 2006.'\n",
      "Feedback: 'The generated answer is relevant and fully correct, providing the correct year of 2006. However, it includes additional information about the specific month of September, which is not necessary for the user query.'\n",
      "\n",
      "Joint (True): 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in September 2006.'\n",
      "Feedback: 'The generated answer is relevant and fully correct, providing the correct year of 2006. However, it includes additional information about the specific month of September, which is not necessary for the user query.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'What is the length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged?'\n",
      "Answer '6.213 km long'\n",
      "\n",
      "KG (False): 'The length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged is not provided in the given context information.'\n",
      "Feedback: 'The generated answer is not relevant to the user query as it does not provide the length of the track.'\n",
      "\n",
      "Text (False): 'The length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged is not provided in the given context information.'\n",
      "Feedback: 'The generated answer is not relevant to the user query as it does not provide the length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged.'\n",
      "\n",
      "Joint (False): 'The length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged is not provided in the given context information.'\n",
      "Feedback: 'The generated answer is not relevant to the user query as it does not provide the length of the track.'\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(query_answer):\n",
    "    print('-------------\\nQuery: %s\\nAnswer %s\\n' % (repr(x['query']), repr(x['answer']) ))\n",
    "\n",
    "    print('KG (%s): %s\\nFeedback: %s\\n' % (repr(kg_res[i].passing), repr(kg_res[i].response), repr(kg_res[i].feedback)))\n",
    "    \n",
    "    print('Text (%s): %s\\nFeedback: %s\\n' % (repr(txt_res[i].passing), repr(txt_res[i].response), repr(txt_res[i].feedback)))\n",
    "    \n",
    "    print('Joint (%s): %s\\nFeedback: %s\\n---------------\\n' % (repr(joint_res[i].passing), repr(joint_res[i].response), repr(joint_res[i].feedback)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0500cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG Correct: 2, Text Correct: 5, Joint Correct: 6, Total: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"KG Correct: {kg_tot}, Text Correct: {txt_tot}, Joint Correct: {joint_tot}, Total: {len(query_answer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40d970",
   "metadata": {},
   "source": [
    "#### Things to do...\n",
    "\n",
    "Looking at the output above, it looks like some correct answers are being marked incorrectly based on the answer being \"too verbose\". We can play with this by changing the prompt for the evaluator. Here are some thoughts on things to try out: \n",
    "\n",
    "    1) Clean up the triples a bit, some of the text was messy and try this again.\n",
    "    2) Look at correcting the prompt for the evaluators.\n",
    "    3) Check if top documents (subgraphs) correspond to the `hotpot_qa` dataset suggestion for top context. \n",
    "    4) Use manually defined embeddings for the KGs (need to fit hetero gnn and pass as embedding method)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric_env",
   "language": "python",
   "name": "torch_geometric_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
