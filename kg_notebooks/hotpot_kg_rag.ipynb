{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8454731",
   "metadata": {},
   "source": [
    "# KG Index and VectorStore Index on Hotpot QA with LlamaIndex\n",
    "\n",
    "\n",
    "This notebook demonstrates the following:\n",
    "\n",
    "    1) Building a custom `KnowledgeGraphIndex` for extracted triples.\n",
    "    2) How to build a customized `BaseRetriever` to retrieve KG triples and text documents jointly.\n",
    "    3) Automated evaluation of KG RAG based QA using LLMs for evaluation on hotpot qa.\n",
    "    \n",
    "This notebook compares a `VectorIndexRetriever`, `KGTableRetriever`, and joint Retriever for RAG on the `hotpot_qa` dataset. The triples can be extracted following `hotpot_qa_extraction.ipynb` or by running `hotpot_qa_kgs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "011ff10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "open_ai_key = '...'\n",
    "os.environ['OPENAI_API_KEY'] = open_ai_key\n",
    "\n",
    "sys.path = ['/Users/walder2/kg_uq/'] + sys.path\n",
    "path_to_data = '/Users/walder2/kg_uq/hotpot_qa_data'\n",
    "\n",
    "from hotpot_qa_data.hotpot_data_load import load_hotpot_kgs\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from llama_index import (\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    KnowledgeGraphIndex,\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    "    QueryBundle,\n",
    "    Response\n",
    ")\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index.schema import NodeWithScore\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "# Retrievers\n",
    "from llama_index.retrievers import (\n",
    "    BaseRetriever,\n",
    "    VectorIndexRetriever,\n",
    "    KGTableRetriever,\n",
    ")\n",
    "\n",
    "#Evaluators\n",
    "from llama_index.evaluation import CorrectnessEvaluator, BaseEvaluator\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import time\n",
    "import asyncio \n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3735a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg, query_answer = load_hotpot_kgs(path_to_data=path_to_data, query_answer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00f20f",
   "metadata": {},
   "source": [
    "### Look at the data\n",
    "\n",
    "`doc_id` is the id for the group (question group id), `sub_idx` is the id of the subgraph extracted for context entry `j` for a particular question. Some of the text is messy, we can clean that up at a later time. \n",
    "\n",
    "Also note that the `file_path` is included. This helps track where the triples came from. It is important to know if you want to track down top-matching subgraphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6cd8c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>head_type</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "      <th>tail_type</th>\n",
       "      <th>file_path</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sub_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>isFirstPrivateFMStationIn</td>\n",
       "      <td>India</td>\n",
       "      <td>place</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>wasStartedOn</td>\n",
       "      <td>3 July 2001</td>\n",
       "      <td>event</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>broadcastsOn</td>\n",
       "      <td>91.1 megahertz</td>\n",
       "      <td>measurement</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>broadcastsFrom</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>place</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radio City</td>\n",
       "      <td>place</td>\n",
       "      <td>broadcastsFrom</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>place</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>Jeff Harris</td>\n",
       "      <td>person</td>\n",
       "      <td>hasOccupation</td>\n",
       "      <td>attorney</td>\n",
       "      <td>person</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Jeff Harris</td>\n",
       "      <td>person</td>\n",
       "      <td>hasOccupation</td>\n",
       "      <td>politician</td>\n",
       "      <td>person</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>Jeff Harris</td>\n",
       "      <td>person</td>\n",
       "      <td>represents</td>\n",
       "      <td>23rd District of Missouri</td>\n",
       "      <td>place</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>Jeff Harris</td>\n",
       "      <td>person</td>\n",
       "      <td>ranFor</td>\n",
       "      <td>attorney general</td>\n",
       "      <td>person</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>Jeff Harris</td>\n",
       "      <td>person</td>\n",
       "      <td>servedAs</td>\n",
       "      <td>Minority Floor Leader</td>\n",
       "      <td>person</td>\n",
       "      <td>/Users/walder2/kg_uq/hotpot_qa_data/txt_files/...</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2001 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             head head_type                   relation  \\\n",
       "0      Radio City     place  isFirstPrivateFMStationIn   \n",
       "1      Radio City     place               wasStartedOn   \n",
       "2      Radio City     place               broadcastsOn   \n",
       "3      Radio City     place             broadcastsFrom   \n",
       "4      Radio City     place             broadcastsFrom   \n",
       "...           ...       ...                        ...   \n",
       "1996  Jeff Harris    person              hasOccupation   \n",
       "1997  Jeff Harris    person              hasOccupation   \n",
       "1998  Jeff Harris    person                 represents   \n",
       "1999  Jeff Harris    person                     ranFor   \n",
       "2000  Jeff Harris    person                   servedAs   \n",
       "\n",
       "                           tail    tail_type  \\\n",
       "0                         India        place   \n",
       "1                   3 July 2001        event   \n",
       "2                91.1 megahertz  measurement   \n",
       "3                        Mumbai        place   \n",
       "4                     Bengaluru        place   \n",
       "...                         ...          ...   \n",
       "1996                   attorney       person   \n",
       "1997                 politician       person   \n",
       "1998  23rd District of Missouri        place   \n",
       "1999           attorney general       person   \n",
       "2000      Minority Floor Leader       person   \n",
       "\n",
       "                                              file_path  doc_id  sub_idx  \n",
       "0     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "1     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "2     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "3     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "4     /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...       0        0  \n",
       "...                                                 ...     ...      ...  \n",
       "1996  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...      19        9  \n",
       "1997  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...      19        9  \n",
       "1998  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...      19        9  \n",
       "1999  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...      19        9  \n",
       "2000  /Users/walder2/kg_uq/hotpot_qa_data/txt_files/...      19        9  \n",
       "\n",
       "[2001 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780848f3",
   "metadata": {},
   "source": [
    "# Building a KnowledgeGraphIndex from extracted triples\n",
    "\n",
    "Pick an LLM for use, default is gpt-3.5-turbo. `service_context` will help with chunking documents and determining with LLM to call. The path for `documents` should point to `'./hotpot_qa_data/txt_files'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb45b6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
    "documents = SimpleDirectoryReader(path_to_data + '/txt_files').load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777d708",
   "metadata": {},
   "source": [
    "Define the and empty `KnowledgeGraphIndex`. We will fill this store up with our extracted triples and a reference to the `Node` with contains the document the triples were extracted from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57494acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_index = KnowledgeGraphIndex(\n",
    "    [],\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "node_parser = SentenceSplitter()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "file_to_node = {node.metadata['file_path']: k for k, node in enumerate(nodes)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e727b6dd",
   "metadata": {},
   "source": [
    "We fill up the `KnowledgeGraphIndex` object by passing in triples corresponding to the `Node` they were extracted from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88f7a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id in kg['doc_id'].unique():\n",
    "    idx = kg['doc_id'] == doc_id\n",
    "\n",
    "    for sub_id in kg[idx]['sub_idx'].unique():\n",
    "        tmp = kg[np.bitwise_and(idx, kg['sub_idx'] == sub_id)]\n",
    "        for h, r, t, f in zip(tmp['head'], tmp['relation'], tmp['tail'], tmp['file_path']):\n",
    "            kg_index.upsert_triplet_and_node((h, r, t), nodes[file_to_node[f]])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735488d",
   "metadata": {},
   "source": [
    "# Define JointReriever for KG triple and text indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99a4b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever that performs both Vector search and Knowledge Graph search\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_retriever: VectorIndexRetriever,\n",
    "        kg_retriever: KGTableRetriever,\n",
    "        mode: str = \"OR\",\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._kg_retriever = kg_retriever\n",
    "        if mode not in (\"AND\", \"OR\"):\n",
    "            raise ValueError(\"Invalid mode.\")\n",
    "        self._mode = mode\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve nodes given query.\"\"\"\n",
    "\n",
    "        vector_nodes = self._vector_retriever.retrieve(query_bundle)\n",
    "        kg_nodes = self._kg_retriever.retrieve(query_bundle)\n",
    "\n",
    "        vector_ids = {n.node.node_id for n in vector_nodes}\n",
    "        kg_ids = {n.node.node_id for n in kg_nodes}\n",
    "\n",
    "        combined_dict = {n.node.node_id: n for n in vector_nodes}\n",
    "        combined_dict.update({n.node.node_id: n for n in kg_nodes})\n",
    "\n",
    "        if self._mode == \"AND\":\n",
    "            retrieve_ids = vector_ids.intersection(kg_ids)\n",
    "        else:\n",
    "            retrieve_ids = vector_ids.union(kg_ids)\n",
    "\n",
    "        retrieve_nodes = [combined_dict[rid] for rid in retrieve_ids]\n",
    "        return retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675accee",
   "metadata": {},
   "source": [
    "Create a `VectorStoreIndex` object for RAG with just the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7575902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfa844",
   "metadata": {},
   "source": [
    "Now we instantiate a retriever for the KQ, text, which is passed in the `CustomRetriever` object for joint retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2ca0f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_template = PromptTemplate(\n",
    "    \"Context information is\"\n",
    "    \" below.\\n---------------------\\n{context_str}\\n---------------------\\nUsing\"\n",
    "    \" only the context information, answer\"\n",
    "    \" the question: {query_str}.\"\n",
    "    \"Be as concise as possible.\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ff812cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom retriever\n",
    "vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "kg_retriever = KGTableRetriever(\n",
    "    index=kg_index, retriever_mode=\"keyword\", include_text=False\n",
    ")\n",
    "joint_retriever = JointRetriever(vector_retriever, kg_retriever)\n",
    "\n",
    "# create response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    service_context=service_context,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    text_qa_template=qa_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01e498",
   "metadata": {},
   "source": [
    "Create query engines for all three cases: text, KG, and joint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f66e39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_query_engine = RetrieverQueryEngine(\n",
    "    retriever=joint_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine(text_qa_template=qa_template)\n",
    "\n",
    "# only use triples from the KG\n",
    "kg_keyword_query_engine = kg_index.as_query_engine(\n",
    "    include_text=False,\n",
    "    retriever_mode=\"keyword\",\n",
    "    response_mode=\"tree_summarize\",\n",
    "    text_qa_template=qa_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d68257",
   "metadata": {},
   "source": [
    "# Evaluation of responses with RAG\n",
    "\n",
    "Define a `CorrectnessEvaluator` that checks correctness of response to the query (with answer supplied). There are other tools for out in the wild evaluation of responses. E.g. \n",
    "\n",
    "`ResponseSourceEvaluator` - uses an LLM to decide if the response is similar enough to the sources -- a good measure for hallunication detection.\n",
    "\n",
    "`QueryResponseEvaluator` - uses an LLM to decide if a response is similar enough to the original query -- a good measure for checking if the query was answered.\n",
    "\n",
    "\n",
    "I've defined a function which uses `CorrectnessEvaluator` to check if the response contains an answer suitable for the query, given the correct answer. We can actually write custom evaluators that fit specified guidelines. This will come in handy later when we want to extend the QA to self defined embeddings ect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ff7e1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import EvaluationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b82d6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_query(query_engine: RetrieverQueryEngine, x: Dict[str, str]):\n",
    "    return x, await query_engine.aquery(x['query'])\n",
    "\n",
    "def run_queries(query_engine: RetrieverQueryEngine, queries: List[Dict[str, str]]):\n",
    "    responses = []\n",
    "    for batch_size in range(0, len(queries), 5):\n",
    "        batch_queries = queries[batch_size:batch_size+5]\n",
    "        \n",
    "        tasks = [run_query(query_engine, x) for x in batch_queries]\n",
    "        responses.extend(asyncio.run(asyncio.gather(*tasks)))\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "240173de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG responses complete...\n",
      "Text responses complete..\n",
      "Joint responses complete..\n"
     ]
    }
   ],
   "source": [
    "kg_res = run_queries(kg_keyword_query_engine, query_answer)\n",
    "print('KG responses complete...')\n",
    "time.sleep(1)\n",
    "txt_res = run_queries(vector_query_engine, query_answer)\n",
    "print('Text responses complete...')\n",
    "time.sleep(1)\n",
    "joint_res = run_queries(joint_query_engine, query_answer)\n",
    "print('Joint responses complete...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "eb2f268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRECTNESS_SYS_TMPL = \"\"\"\n",
    "You are an expert evaluation system for a question answering chatbot.\n",
    "\n",
    "You are given the following information:\n",
    "- a user query, \n",
    "- a reference answer, and\n",
    "- a generated answer.\n",
    "\n",
    "Your job is to judge the relevance and correctness of the generated answer.\n",
    "Output a score that represents a holistic evaluation and a concise explanation reason for the score.\n",
    "\n",
    "\n",
    "Follow these guidelines for scoring:\n",
    "- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\n",
    "- If the generated answer is not relevant to the user query, \\\n",
    "you should give a score of 1.\n",
    "- If the generated answer is relevant but contains mistakes, \\\n",
    "you should give a score between 2 and 3.\n",
    "- If the generated answer is relevant and fully correct, \\\n",
    "you should give a score between 4 and 5.\n",
    "- The length of the answer should not affect the score at all.\n",
    "\n",
    "--> Begining of example\n",
    "# query \n",
    "\"Which magazine was started first Arthur's Magazine or First for Women?\"\n",
    "\n",
    "# reference_answer\n",
    "\"Arthur's Magazine\"\n",
    "\n",
    "# generated_answer\n",
    "\"Arthur's Magazine\"\n",
    "\n",
    "# Output\n",
    "_score_=5.0\n",
    "_reason_=The response matched the reference answer perfectly.\n",
    "\n",
    "--> End of example\n",
    "\n",
    "The output should be returned following the example above only, using only two lines.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_USER_TMPL = \"\"\"\n",
    "## User Query\n",
    "{query}\n",
    "\n",
    "## Reference Answer\n",
    "{reference_answer}\n",
    "\n",
    "## Generated Answer\n",
    "{generated_answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f3c92e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import AsyncOpenAI\n",
    "\n",
    "class HotpotEvaluator:\n",
    "    def __init__(self, user_template: str, system_template: str, threshold: float = 4.0):\n",
    "        self.client = AsyncOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        self.threshold = threshold\n",
    "        self.user_template = user_template\n",
    "        self.system_template = system_template\n",
    "        \n",
    "    async def run_eval(self, x: Tuple[Dict[str, str], Response]) -> str:\n",
    "        completion = await self.client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo',\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_template\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": self.user_template.format(query=x[0]['query'], reference_answer=x[0]['answer'], generated_answer=x[1].response)\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return completion.choices[0].message.content\n",
    "        \n",
    "        \n",
    "    def get_evals(self, qa_responses):\n",
    "        \n",
    "        results = []\n",
    "        for batch_size in range(0, len(qa_responses), 5):\n",
    "            \n",
    "            batch_x = qa_responses[batch_size:batch_size+5]\n",
    "            \n",
    "            tasks = [self.run_eval(x) for x in batch_x]\n",
    "            rv = asyncio.run(asyncio.gather(*tasks))\n",
    "            \n",
    "            for x in rv:\n",
    "                y = x.split('\\n')\n",
    "                for u in y:\n",
    "                    if '_score_=' in u:\n",
    "                        score = float(u.split('_score_=')[1])\n",
    "                    elif '_reason_=' in u:\n",
    "                        reason = u.split('_reason_=')[1]\n",
    "\n",
    "                results.append({\"passing\": score >= 4.0, \"score\": score, \"reason\": reason})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "13bdbb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = HotpotEvaluator(user_template=CORRECTNESS_USER_TMPL, system_template=CORRECTNESS_SYS_TMPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed22b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_eval = evaluator.get_evals(kg_res)\n",
    "print('KG evaluations complete...')\n",
    "time.sleep(1)\n",
    "txt_res = evaluator.get_evals(txt_res)\n",
    "print('Text evaluations complete...')\n",
    "time.sleep(1)\n",
    "joint_res = evaluator.get_evals(joint_res)\n",
    "print('Joint evaluations complete...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f5f82",
   "metadata": {},
   "source": [
    "Run the retrievers on the queries and check correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccf2eb",
   "metadata": {},
   "source": [
    "Take a look at the results. The LLM will tell us if the retrieval is correct and some feedback on why the response was deemed correct or incorrect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "23a35c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Query: \"Which magazine was started first Arthur's Magazine or First for Women?\"\n",
      "Answer \"Arthur's Magazine\"\n",
      "\n",
      "KG (True): \"Arthur's Magazine was started first.\"\n",
      "Feedback: 'The response is relevant and fully correct, matching the reference answer perfectly.'\n",
      "\n",
      "Text (True): \"Arthur's Magazine was started before First for Women.\"\n",
      "Feedback: \"The generated answer is both relevant and correct, providing the accurate information that Arthur's Magazine was started before First for Women.\"\n",
      "\n",
      "Joint (True): \"Arthur's Magazine was started before First for Women.\"\n",
      "Feedback: \"The generated answer is both relevant and correct, providing the accurate information that Arthur's Magazine was started before First for Women.\"\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'The Oberoi family is part of a hotel company that has a head office in what city?'\n",
      "Answer 'Delhi'\n",
      "\n",
      "KG (True): 'Delhi'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Text (True): 'The Oberoi family is part of a hotel company that has a head office in Delhi.'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Joint (True): 'Delhi'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?'\n",
      "Answer 'President Richard Nixon'\n",
      "\n",
      "KG (False): 'Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after members of his own family.'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake as Matt Groening named the character Milhouse after President Richard Nixon, not members of his own family.'\n",
      "\n",
      "Text (True): \"President Richard Nixon's middle name.\"\n",
      "Feedback: 'The generated answer is relevant and correct, but it is missing the specific detail that Matt Groening named the character Milhouse after President Richard Nixon.'\n",
      "\n",
      "Joint (True): \"President Richard Nixon's middle name.\"\n",
      "Feedback: \"The generated answer is relevant but contains a mistake by mentioning Nixon's middle name instead of the full name.\"\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: \" What nationality was James Henry Miller's wife?\"\n",
      "Answer 'American'\n",
      "\n",
      "KG (False): \"There is no information provided about James Henry Miller's wife in the given context.\"\n",
      "Feedback: 'The generated answer is not relevant to the user query.'\n",
      "\n",
      "Text (False): \"The context information does not provide any details about James Henry Miller's wife.\"\n",
      "Feedback: 'The generated answer is not relevant to the user query.'\n",
      "\n",
      "Joint (False): \"The nationality of James Henry Miller's wife is not provided in the given context information.\"\n",
      "Feedback: 'The generated answer is not relevant to the user query.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Cadmium Chloride is slightly soluble in this chemical, it is also called what?'\n",
      "Answer 'alcohol'\n",
      "\n",
      "KG (False): 'Ammonium chloride'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake as \"Ammonium chloride\" is not the correct answer, the correct answer is \"alcohol\".'\n",
      "\n",
      "Text (True): 'alcohol'\n",
      "Feedback: 'The generated answer is relevant and matches the reference answer perfectly.'\n",
      "\n",
      "Joint (True): 'alcohol'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Which tennis player won more Grand Slam titles, Henri Leconte or Jonathan Stark?'\n",
      "Answer 'Jonathan Stark'\n",
      "\n",
      "KG (False): 'Henri Leconte won more Grand Slam titles than Jonathan Stark.'\n",
      "Feedback: 'The generated answer is relevant to the user query but contains a mistake as Jonathan Stark actually won more Grand Slam titles.'\n",
      "\n",
      "Text (True): 'Jonathan Stark won more Grand Slam titles than Henri Leconte.'\n",
      "Feedback: 'The response is relevant and fully correct, matching the reference answer.'\n",
      "\n",
      "Joint (True): 'Jonathan Stark won more Grand Slam titles than Henri Leconte.'\n",
      "Feedback: 'The generated answer is relevant and fully correct, matching the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: \"Which genus of moth in the world's seventh-largest country contains only one species?\"\n",
      "Answer 'Crambidae'\n",
      "\n",
      "KG (False): \"The genus of moth in the world's seventh-largest country that contains only one species is Hepialus.\"\n",
      "Feedback: 'The generated answer is not relevant to the user query.'\n",
      "\n",
      "Text (False): 'Nymphuliella.'\n",
      "Feedback: 'The generated answer is relevant but incorrect. The correct answer should be Crambidae.'\n",
      "\n",
      "Joint (False): \"Yoshiyasua is the genus of moth in the world's seventh-largest country that contains only one species.\"\n",
      "Feedback: 'The generated answer is relevant but contains a mistake as the correct genus of moth is Crambidae, not Yoshiyasua.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Who was once considered the best kick boxer in the world, however he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring.'\n",
      "Answer 'Badr Hari'\n",
      "\n",
      "KG (True): 'Badr Hari was once considered the best kickboxer in the world, but he has been involved in a number of controversies relating to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring.'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Text (True): 'Badr Hari'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Joint (True): 'Badr Hari was once considered the best kickboxer in the world, but he has been involved in a number of controversies related to his \"unsportsmanlike conducts\" in the sport and crimes of violence outside of the ring.'\n",
      "Feedback: 'The generated answer is both relevant and correct, matching the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in what year?'\n",
      "Answer '2006'\n",
      "\n",
      "KG (True): 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in 2006.'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Text (True): 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in September 2006.'\n",
      "Feedback: 'The generated answer is relevant and correct, but it includes unnecessary specific information about September.'\n",
      "\n",
      "Joint (True): 'The Dutch-Belgian television series that \"House of Anubis\" was based on first aired in September 2006.'\n",
      "Feedback: 'The generated answer is relevant and correct, but it contains additional unnecessary information (September).'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'What is the length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged?'\n",
      "Answer '6.213 km long'\n",
      "\n",
      "KG (True): 'The length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged is 6.213 kilometers.'\n",
      "Feedback: 'The generated answer is both relevant and correct, matching the reference answer perfectly.'\n",
      "\n",
      "Text (False): 'The length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged is Mount Panorama Circuit.'\n",
      "Feedback: 'The generated answer is relevant but incorrect as it does not provide the specific length of the track.'\n",
      "\n",
      "Joint (False): 'The length of the track where the 2013 Liqui Moly Bathurst 12 Hour was staged is Mount Panorama Circuit.'\n",
      "Feedback: 'The generated answer is relevant but incorrect as it does not provide the specific length of the track where the event was staged.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Fast Cars, Danger, Fire and Knives includes guest appearances from which hip hop record executive?'\n",
      "Answer 'Jaime Meline'\n",
      "\n",
      "KG (False): 'Damon Dash'\n",
      "Feedback: 'The generated answer is relevant to the query but incorrect. The correct answer should be \"Jaime Meline\".'\n",
      "\n",
      "Text (False): 'El-P.'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake as the correct hip hop record executive is Jaime Meline, not El-P.'\n",
      "\n",
      "Joint (True): 'El-P'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake as El-P is the stage name of Jaime Meline, the correct answer.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Gunmen from Laredo starred which narrator of \"Frontier\"?'\n",
      "Answer 'Walter Darwin Coy'\n",
      "\n",
      "KG (False): 'The narrator of \"Frontier\" from Laredo is Gunmen.'\n",
      "Feedback: 'The generated answer is relevant to the user query but contains a mistake by stating \"Gunmen\" as the narrator instead of \"Walter Darwin Coy\".'\n",
      "\n",
      "Text (False): 'Robert Knapp'\n",
      "Feedback: 'The generated answer is not relevant to the user query.'\n",
      "\n",
      "Joint (False): 'Robert Knapp'\n",
      "Feedback: 'The generated answer is not relevant to the user query.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Where did the form of music played by Die Rhöner Säuwäntzt originate?'\n",
      "Answer 'United States'\n",
      "\n",
      "KG (False): 'The form of music played by Die Rhöner Säuwäntzt originated in Eichenzell-Lütter.'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake as it states the music originated in Eichenzell-Lütter instead of the United States.'\n",
      "\n",
      "Text (False): 'The form of music played by Die Rhöner Säuwäntzt originated in Hessen, Germany.'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake as it states the music originated in Hessen, Germany instead of the United States.'\n",
      "\n",
      "Joint (False): 'The form of music played by Die Rhöner Säuwäntzt originated in the Rhön Mountains dialect and other Hessian dialects varieties.'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake as it states the music originated in the Rhön Mountains dialect and other Hessian dialects varieties, while the correct origin is the United States.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'In which American football game was Malcolm Smith named Most Valuable player?'\n",
      "Answer 'Super Bowl XLVIII'\n",
      "\n",
      "KG (True): 'Malcolm Smith was named Most Valuable Player in Super Bowl XLVIII.'\n",
      "Feedback: 'The generated answer is both relevant and correct, matching the reference answer perfectly.'\n",
      "\n",
      "Text (True): 'Super Bowl XLVIII.'\n",
      "Feedback: 'The generated answer is both relevant and correct, matching the reference answer perfectly.'\n",
      "\n",
      "Joint (True): 'Malcolm Smith was named the Most Valuable Player in Super Bowl XLVIII.'\n",
      "Feedback: 'The generated answer is both relevant and correct, matching the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'What U.S Highway gives access to Zilpo Road, and is also known as Midland Trail?'\n",
      "Answer 'US 60'\n",
      "\n",
      "KG (True): 'U.S. Highway 60'\n",
      "Feedback: 'The generated answer is both relevant and correct, matching the reference answer perfectly.'\n",
      "\n",
      "Text (True): 'U.S. Highway 60.'\n",
      "Feedback: 'The generated answer is both relevant and correct, matching the reference answer perfectly.'\n",
      "\n",
      "Joint (True): 'U.S. Highway 60 gives access to Zilpo Road and is also known as Midland Trail.'\n",
      "Feedback: 'The generated answer is both relevant and fully correct, matching the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'The 1988 American comedy film, The Great Outdoors, starred a four-time Academy Award nominee, who received a star on the Hollywood Walk of Fame in what year?'\n",
      "Answer '2006'\n",
      "\n",
      "KG (False): 'The 1988 American comedy film, \"The Great Outdoors,\" starred a four-time Academy Award nominee, Dan Aykroyd, who received a star on the Hollywood Walk of Fame in 2011.'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake in the year Dan Aykroyd received a star on the Hollywood Walk of Fame.'\n",
      "\n",
      "Text (False): 'The star of The Great Outdoors, John Candy, received a star on the Hollywood Walk of Fame in 2003.'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake in the year John Candy received a star on the Hollywood Walk of Fame.'\n",
      "\n",
      "Joint (False): 'The four-time Academy Award nominee who starred in the 1988 American comedy film \"The Great Outdoors\" received a star on the Hollywood Walk of Fame in 2003.'\n",
      "Feedback: 'The generated answer is relevant but contains a mistake in the year the star was received on the Hollywood Walk of Fame.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'What are the names of the current members of  American heavy metal band who wrote the music for  Hurt Locker The Musical? '\n",
      "Answer 'Hetfield and Ulrich, longtime lead guitarist Kirk Hammett, and bassist Robert Trujillo.'\n",
      "\n",
      "KG (False): 'Jaime St. James, Patrick Young, Pete Holmes, Bob Capka, and Brandon Cook are the names of the current members of the American heavy metal band who wrote the music for Hurt Locker The Musical.'\n",
      "Feedback: 'The generated answer is relevant but contains mistakes as the names provided do not match the reference answer.'\n",
      "\n",
      "Text (True): 'The current members of Metallica wrote the music for Hurt Locker The Musical.'\n",
      "Feedback: 'The generated answer is relevant and correct, but it incorrectly refers to the band as \"Metallica\" instead of specifying the names of the members.'\n",
      "\n",
      "Joint (True): 'James Hetfield, Lars Ulrich, Kirk Hammett, and Robert Trujillo'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Human Error\" is the season finale of the third season of a tv show that aired on what network?'\n",
      "Answer 'Fox'\n",
      "\n",
      "KG (True): 'Fox network'\n",
      "Feedback: 'The generated answer is relevant and correct, but contains an unnecessary word \"network\".'\n",
      "\n",
      "Text (True): '\"Human Error\" is the season finale of the third season of \"House\" that aired on Fox.'\n",
      "Feedback: 'The generated answer is relevant and fully correct, matching the reference answer perfectly.'\n",
      "\n",
      "Joint (True): 'Fox network'\n",
      "Feedback: 'The response is relevant and fully correct, matching the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'Dua Lipa, an English singer, songwriter and model, the album spawned the number-one single \"New Rules\" is a song by English singer Dua Lipa from her eponymous debut studio album, released in what year?'\n",
      "Answer '2017'\n",
      "\n",
      "KG (True): '2017'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Text (True): '2017'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Joint (True): '2017'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "---------------\n",
      "\n",
      "-------------\n",
      "Query: 'American politician Joe Heck ran unsuccessfully against Democrat Catherine Cortez Masto, a woman who previously served as the 32nd Attorney General of where?'\n",
      "Answer 'Nevada'\n",
      "\n",
      "KG (True): 'Nevada'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Text (True): 'Nevada'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "\n",
      "Joint (True): 'Nevada'\n",
      "Feedback: 'The response matched the reference answer perfectly.'\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kg_tot, txt_tot, joint_tot = 0, 0, 0\n",
    "for i, x in enumerate(query_answer):\n",
    "    print('-------------\\nQuery: %s\\nAnswer %s\\n' % (repr(x['query']), repr(x['answer']) ))\n",
    "    \n",
    "    print('KG (%s): %s\\nFeedback: %s\\n' % (repr(kg_eval[i]['passing']), repr(kg_res[i][1].response), repr(kg_eval[i]['reason'])))\n",
    "    \n",
    "    print('Text (%s): %s\\nFeedback: %s\\n' % (repr(txt_eval[i]['passing']), repr(txt_res[i][1].response), repr(txt_eval[i]['reason'])))\n",
    "    \n",
    "    print('Joint (%s): %s\\nFeedback: %s\\n---------------\\n' % (repr(joint_eval[i]['passing']), repr(joint_res[i][1].response), repr(joint_eval[i]['reason'])))\n",
    "    \n",
    "    kg_tot += kg_eval[i]['passing']\n",
    "    txt_tot += txt_eval[i]['passing']\n",
    "    joint_tot += joint_eval[i]['passing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "d0500cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG Correct: 10, Text Correct: 13, Joint Correct: 14, Total: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"KG Correct: {kg_tot}, Text Correct: {txt_tot}, Joint Correct: {joint_tot}, Total: {len(query_answer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40d970",
   "metadata": {},
   "source": [
    "#### Things to do...\n",
    "\n",
    "Looking at the output above, it looks like some correct answers are being marked incorrectly based on the answer being \"too verbose\". We can play with this by changing the prompt for the evaluator. Here are some thoughts on things to try out: \n",
    "\n",
    "    1) Clean up the triples a bit, some of the text was messy and try this again.\n",
    "    2) Look at correcting the prompt for the evaluators.\n",
    "    3) Check if top documents (subgraphs) correspond to the `hotpot_qa` dataset suggestion for top context. \n",
    "    4) Use manually defined embeddings for the KGs (need to fit hetero gnn and pass as embedding method)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric_env",
   "language": "python",
   "name": "torch_geometric_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
