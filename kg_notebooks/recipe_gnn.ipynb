{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0071183d",
   "metadata": {},
   "source": [
    "# Load extracted KG recipes and fit a GNN via PyTorch Geometric\n",
    "\n",
    "This notebook provides an example of how to load in the extracted triples from https://www.allrecipes.com and fit them to a heterogenous GNN via pytorch geometric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9303f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walder2/torch_geometric_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import torch as tn\n",
    "import torch.optim \n",
    "from torch.nn import ModuleDict \n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import to_hetero, Linear, GATConv, HeteroConv\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "sys.path = ['/Users/walder2/kg_uq/'] + sys.path\n",
    "path_to_data = '/Users/walder2/kg_uq/recipe_data'\n",
    "\n",
    "from kg_extraction import kg_to_hetero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff0242",
   "metadata": {},
   "source": [
    "# Define the Hetero GNN\n",
    "\n",
    "This is just one example. You can head to the docs here https://pytorch-geometric.readthedocs.io/en/latest/cheatsheet/gnn_cheatsheet.html and find heterogenous GNNs that handle features (and possibly lazy-initialization).\n",
    "\n",
    "The GNN maps from `HeteroData()` to $\\mathbb{R}^d$, with a linear layer at the bottom aggregating from each node type. If you change this, just be mindful of the loss function used later on. \n",
    "\n",
    "Note that the emebeddings for the features are `torch.float32`, so you need to be mindful of how you initialize the variables of the GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9b294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sig = torch.nn.LogSigmoid()\n",
    "\n",
    "class HeteroGNN(tn.nn.Module):\n",
    "    def __init__(self, hidden_channels, emb_dim, num_layers, edge_types, node_types):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = emb_dim\n",
    "        self.convs = tn.nn.ModuleList()\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({key : GATConv((-1, -1), hidden_channels, add_self_loops=False) for key in edge_types}, aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "            \n",
    "        self.lin_dict = ModuleDict({x: Linear(hidden_channels, emb_dim, bias=False) for x in node_types})\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        rv = tn.zeros(self.emb_dim)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: x.relu() for key, x in x_dict.items()}\n",
    "            \n",
    "        # may want to change the aggregation layer here\n",
    "        for key, x in x_dict.items():\n",
    "            rv += self.lin_dict[key](torch.mean(x, dim=0))\n",
    "        return rv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c604ac",
   "metadata": {},
   "source": [
    "# Load the data \n",
    "\n",
    "The returned dictionary contains:\n",
    "\n",
    "    - 'train': a list of `HeteroData()` objects (one for each subgraph)\n",
    "    - 'kg': the KG as a `DataFrame`\n",
    "    - 'embedding_maps': Tuple of node type sentence enumeration, and corresponding node type embeddings\n",
    "    \n",
    "You need to specify the path to `./recipe_data` in `data_dir`. You can speficy an embedding model for the sentences by passing a string to `sentence_transformer_model`, which corresponds to a type of embeddings for `SentenceTransformer` objects (see https://www.sbert.net/). `Passing `undirected = True`, supplies reverse edges for fitting. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90969aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = kg_to_hetero(data_dir=path_to_data, sentence_transformer_model='all-MiniLM-L6-v2', undirected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5648b9f",
   "metadata": {},
   "source": [
    "# Initialize the GNN\n",
    "\n",
    "`emb_dim` is the dimension mapped to by the encoder. You need to specify the edge types and node types to initialize the GNN as defined. \n",
    "\n",
    "\n",
    "Model is fit by performing an N-ary classification. Each graph is embedded to get $f_{\\theta}(G_i) = \\boldsymbol{d}_i$. The optimization maximizes \n",
    "\n",
    "$$ \\sum_{i=1}^{N} P(G_i \\vert D ) = \\sum_{i=1}^{n} \\frac{\\sigma(\\boldsymbol{d}_i^{T}\\boldsymbol{d}_i)}{\\sum_{i=1}^{N}  \\sigma(\\boldsymbol{d}_i^{T}\\boldsymbol{d}_j) }, $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$ is a sigmoid function This is not a great way to do this, but for a first pass try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab06652",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 10 \n",
    "\n",
    "edge_types = tuple(set((y for x in train for y in x.edge_types)))\n",
    "node_types = tuple((y for x in train for y in x.node_types))\n",
    "\n",
    "encoder = HeteroGNN(hidden_channels=16, num_layers=2, emb_dim=emb_dim, edge_types=edge_types, node_types=node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df8f49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss tensor(11.3430)\n",
      "Epoch 20, loss tensor(11.3430)\n",
      "Epoch 30, loss tensor(11.3430)\n",
      "Epoch 40, loss tensor(11.3430)\n",
      "Epoch 50, loss tensor(11.3430)\n",
      "Epoch 60, loss tensor(11.3430)\n",
      "Epoch 70, loss tensor(11.3430)\n",
      "Epoch 80, loss tensor(11.3430)\n",
      "Epoch 90, loss tensor(11.3430)\n",
      "Epoch 100, loss tensor(11.3430)\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "ntrain = len(train)\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.90)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    rv = tn.zeros((ntrain, emb_dim))\n",
    "    \n",
    "    for i in range(ntrain):\n",
    "        rv[i] = encoder(train[i].x_dict, train[i].edge_index_dict)\n",
    "        \n",
    "    d_ij = log_sig(tn.matmul(rv, rv.T))\n",
    "    loss = -tn.sum(tn.diag(d_ij) - tn.logsumexp(d_ij, dim=1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch %s, loss %s' % (repr(epoch+1), repr(loss.detach())))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b4c65",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "Extract more KGs, maybe consider a different schema. We'd like to add some noise via removing/add edges. It would also be nice to make sure that similar recipes with different extraction types are embedded close together. We should provide some search depth curves on this and start thinking about a better fit and UQ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e769c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric_env",
   "language": "python",
   "name": "torch_geometric_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
